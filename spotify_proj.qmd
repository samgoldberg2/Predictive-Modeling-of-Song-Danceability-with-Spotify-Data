---
author: "Sam Goldberg (shg59)"
date: today
format: pdf
editor: visual
execute:
  warning: false
  cache: true
---

# Setup

Load packages and data:

```{r}
#| label: load-packages
#| cache: false
# label: load-packages
# cache: false

library(tidyverse)
library(tidymodels)
```

# Part 1

## Exercise 1

Three variables I think would be predictive of the median debt load for a student graduating from a four year college or university:

cost_net: The cost of attending the institution will definitely have a relationship with student debt because a higher cost of attending will probably cause a higher median student debt.

median_hh_inc: It is more likely for students from lower-income households to take out loans which causes a higher median student debt.

pell_pct: A Pell grant, which comes from the U.S. government, is a grant for low-income students who show a financial need for it and have to pay for college. A pell_pct that is higher likely means that there are more low-income students, which means they need to take out more loans, leading to more student debt.

```{r}
#| label: median-debt-load-variables
#| fig-width: 7
#| fig-height: 5
# label: median-debt-load-variables
# fig-width: 7
# fig-height: 5

# add code here

scorecard_train <- readRDS('data/scorecard-train.rds')
scorecard_test <- readRDS('data/scorecard-test.rds')

scorecard_train |> 
  select(cost_net, pell_pct, median_hh_inc, debt) |>
  pivot_longer(-debt, names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = debt)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~variable, scales = "free") +
  labs(x = "Variable Value", 
       y = "Median Debt", 
       title = "Selected Variables vs. Median Debt") +
  theme_minimal()
```

From these faceted plots, one can see the correlations between the chosen variables and the median debt load. For example, it appears that a higher cost_net will result in a higher median debt load, a higher median_hh_inc will result in a higher median debt load, and a higher pell_pct will result in a lower median debt load.

## Exercise 2

```{r}
#| label: null-model
# label: null-model

# add code here

set.seed(100)

scorecard_folds <- vfold_cv(data = scorecard_train, 
                            v = 10, strata = debt)

 null_spec <- null_model() |>
   set_engine("parsnip") |> 
   set_mode("regression") |> 
   translate()
 
 null_fit <- null_spec |>
   fit_resamples(
     preprocessor = debt ~ .,
     resamples = scorecard_folds) |>
   collect_metrics()

null_rmse <- null_fit |>
  filter(.metric == "rmse") 

null_rmse
  
```

The mean RMSE is 4810.108. This means that on average, the null model's predictions are off by about \$4810.108 in predicting the mean student debt. Since this is a null model, a useful predictive model should have a mean RMSE below this value.

## Exercise 3

```{r}
#| label: linear-regression-model
# label: linear-regression-model

# add code here

set.seed(100)

lm_rec <- recipe(debt ~ cost_net + median_hh_inc + 
                   pell_pct, data = scorecard_train) |>
  step_normalize(all_numeric_predictors(), -all_outcomes()) |>
  step_dummy(all_nominal_predictors(), -all_outcomes())

lm_mod <- linear_reg() |>
  set_engine("lm") |>
  translate()

lm_wf <- workflow() |>
  add_recipe(lm_rec) |>
  add_model(lm_mod)

lm_fit <- lm_wf |>
  fit_resamples(resamples = scorecard_folds) |>
  collect_metrics()

lm_rmse <- lm_fit |>
  filter(.metric == "rmse") 
  
lm_rmse
```

The mean RMSE is 3971.279. This means that on average, the null model's predictions are off by about \$3971.279 in predicting the mean student debt. This value is below the mean RMSE of the null model, which means this is a better predictive model than the null model.

## Exercise 4

```{r}
#| label: decision-tree
# label: decision-tree

# add code here

set.seed(100)

tree_recipe <- recipe(debt ~ cost_net + median_hh_inc + 
                        pell_pct, data = scorecard_train) |>
  step_normalize(all_numeric_predictors(), -all_outcomes()) |>
  step_dummy(all_nominal_predictors(), -all_outcomes())

tree_mod <- decision_tree() |>
  set_mode("regression") |>
  set_engine("rpart")

tree_grid <- grid_regular(min_n(), levels = 10)

tree_wf <- workflow() |>
  add_model(tree_mod) |>
  add_recipe(tree_recipe)

tree_res <- tune_grid(tree_wf, resamples = scorecard_folds, 
                      grid = tree_grid)

tree_metrics <- tree_res |>
  collect_metrics() |>
  filter(.metric == "rmse")

tree_metrics

```

For the decision tree model, I normalized all numeric predictors and then used step_dummy() on all nominal predictors. The model I used is a decision tree, and I set the engine to rpart and mode to regression since debt is a continuous variable. I then tuned the min_n hyperparameter and collected the metrics to observe the RMSE. The mean RMSE for this model is 4048.653.

```{r}
#| label: random-forest
# label: random-forest

set.seed(100)

rf_recipe <- recipe(debt ~ cost_net + median_hh_inc + 
                      pell_pct, data = scorecard_train) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_naomit(all_outcomes()) 

rf_mod <- rand_forest(mtry = tune(), min_n = tune()) |>
  set_mode("regression") |>
  set_engine("ranger")

ctrl_grid <- control_grid(save_workflow = TRUE)

rf_wf <- workflow() |>
  add_model(rf_mod) |>
  add_recipe(rf_recipe)

rf_tune <- rf_wf |>
  tune_grid(resamples = scorecard_folds, grid = 10, 
            control = ctrl_grid)

rf_metrics <- rf_tune |>
  collect_metrics() |>
  filter(.metric == "rmse")

rf_metrics

rf_tune |>
  autoplot()
```

For the random forest model, I utilized median and modal imputation to replace missing values in the cost_net, median_hh_inc, and pell_pct columns with their medians. Additionally, I used the ranger engine for the random forest model and the regression mode since we are predicting debt, which is a continuous variable. I used the tune_grid() function to perform hyperparameter tuning. This involves training models with different combinations of hyperparameters and seeing which combination has the lowest RMSE. The lowest mean RMSE for this model is 3840.319, which is the best performance yet. This occurs at mtry = 2 and min_n = 34.

```{r}
#| label: k-nearest-neighbors
# label: k-nearest-neighbors

set.seed(100)

knn_recipe <- recipe(debt ~ cost_net + median_hh_inc + 
                       pell_pct, data = scorecard_train) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_naomit(all_outcomes()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

knn_mod <- nearest_neighbor() |>
  set_mode("regression") |>
  set_engine("kknn")

knn_grid <- grid_regular(neighbors(), levels = 10)

knn_wf <- workflow() |>
  add_model(knn_mod) |>
  add_recipe(knn_recipe)

knn_tune <- knn_wf |>
  tune_grid(knn_wf, resamples = scorecard_folds, grid = knn_grid)

knn_metrics <- knn_tune |>
  collect_metrics() |>
  filter(.metric == "rmse")

knn_metrics

```

For the k-nearest neighbor model, I know that it requires all numeric predictors, and all need to be centered and scaled. I used step_dummy() to convert nominal data into numeric dummy variables, which was needed as predictors. I used step_novel() since it "adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set" (Soltoff Better training data lecture). I also used step_zv() to handle zero variance variables. I normalized all numeric predictors and then used step_dummy() on all nominal predictors and then utilized median and modal imputation to replace missing values in the cost_net, median_hh_inc, and pell_pct columns with their medians. I used kknn as the engine and tuned the neighbors() hyperparameter, which determines the number of neighbors to consider when making predictions. The mean RMSE for this model is 4174.74.

```{r}
#| label: ridge-linear-regression
# label: ridge-linear-regression

set.seed(100)

ridge_mod <- linear_reg(penalty = tune(), mixture = 1) |>
  set_mode("regression") |>
  set_engine("glmnet")

ridge_grid <- grid_regular(penalty(), levels = 10)

ridge_wf <- workflow() |>
  add_model(ridge_mod) |>
  add_recipe(knn_recipe)

ridge_tune <- tune_grid(ridge_wf, resamples = scorecard_folds, 
                        grid = ridge_grid)

ridge_metrics <- ridge_tune |>
  collect_metrics() |>
  filter(.metric == "rmse")

ridge_metrics
```

For the ridge linear regression model, I used the same recipe as the k-nearest neighbor model. I used glmnet as the engine and tuned the penalty hyperparameter. This controls the amount of regularization in the model. The mean RMSE for this model is 4070.578.

## Exercise 5

```{r}
#| label: final-predictive-model
# label: final-predictive-model

# add code here

rf_mod_final <- rand_forest(mtry = 2, min_n = 34) |>
  set_mode("regression") |>
  set_engine("ranger")

rf_wf_final <- workflow() |>
  add_model(rf_mod_final) |>
  add_recipe(rf_recipe)

rf_fit_final <- rf_wf_final |> 
  fit(data = scorecard_train)

bind_cols(
  scorecard_test,
  predict(rf_fit_final, new_data = scorecard_test)
) |> 
  select(unit_id, .pred) |>
  write_csv(file = "data/scorecard-preds.csv")

```

I chose the random model because it performed the best in terms of RMSE during the cross-validation process. With the hyperparameters set to the optimal levels, this model performs the best in terms of predicting median debt.

# Part 2

## Exercise 6

```{r}
#| label: null-model-danceable
# label: null-model-danceable

# add code here
spotify_train <- readRDS('data/spotify-train.rds')
spotify_test <- readRDS('data/spotify-test.rds')

set.seed(100)

spotify_folds <- vfold_cv(data = spotify_train, v = 10, 
                          strata = danceability)

 null_spotify_spec <- null_model() |>
   set_engine("parsnip") |> 
   set_mode("classification") |> 
   translate()
 
 null_spotify_fit <- null_spotify_spec |>
   fit_resamples(
     preprocessor = danceability ~ .,
     resamples = spotify_folds) |>
   collect_metrics()

null_spotify_fit
```

The accuracy for this model is 0.5661698 while the roc_auc is 0.50000. With a roc_auc of 0.5, the null model performs poorly. This model is worthless, as it is just random guessing.

## Exercise 7

```{r}
#| label: logistic-regression-model
# label: logistic-regression-model

# add code here

set.seed(100)

spotify_lr_recipe <- recipe(danceability ~ energy + liveness + 
                              loudness + speechiness + tempo, 
                            data = spotify_train) |>
  step_novel(all_nominal_predictors()) |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_numeric_predictors())

log_spotify_spec <- logistic_reg() |>
set_engine("glm")

spotify_lr_wf <- workflow() |>
add_recipe(spotify_lr_recipe) |>
add_model(log_spotify_spec)

lr_spotify_fit <- spotify_lr_wf |>
fit_resamples(
resamples = spotify_folds,
control = ctrl_grid
) |>
collect_metrics()

lr_spotify_fit
```

For my predictor variables, I chose energy, liveness, loudness, speechiness, and tempo since I believe these are all factors that have a direct correlation with the danceability of songs. The accuracy for the logistic regression model is 0.6830789 while the roc_auc is 0.7391180. With a roc_auc slightly above 0.7, this model performs decently.

## Exercise 8

```{r}
#| label: spotify-decision-tree
# label: spotify-decision-tree

# add code here

set.seed(100)

spotify_tree_recipe <- recipe(danceability ~ energy + liveness + 
                              loudness + speechiness + tempo, 
                            data = spotify_train) |>
  step_normalize(all_numeric_predictors(), -all_outcomes()) |>
  step_dummy(all_nominal_predictors(), -all_outcomes())

spotify_tree_mod <- decision_tree() |>
  set_mode("classification") |>
  set_engine("rpart")

spotify_tree_grid <- grid_regular(min_n(), levels = 10)

spotify_tree_wf <- workflow() |>
  add_model(spotify_tree_mod) |>
  add_recipe(spotify_tree_recipe)

spotify_tree_res <- tune_grid(spotify_tree_wf, resamples = spotify_folds, 
                      grid = spotify_tree_grid)

spotify_tree_metrics <- spotify_tree_res |>
  collect_metrics()

spotify_tree_metrics
```

For the decision tree model, I normalized all numeric predictors and then used step_dummy() on all nominal predictors. The model I used is a decision tree, and I set the engine to rpart and the mode to classification since danceability has a binary outcome. I then tuned the min_n hyperparameter and collected the metrics to observe the accuracy and ROC AUC. The accuracy for the decision tree model is 0.7213433 while the roc_auc is 0.7259212. With a roc_auc slightly above 0.7, this model performs decently.

```{r}
#| label: spotify-random-forest
# label: spotify-random-forest

set.seed(100)

spotify_rf_recipe <- recipe(danceability ~ energy + liveness + 
                              loudness + speechiness + tempo, 
                            data = spotify_train) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_naomit(all_outcomes()) 

spotify_rf_mod <- rand_forest(mtry = tune(), min_n = tune()) |>
  set_mode("classification") |>
  set_engine("ranger")

spotify_rf_wf <- workflow() |>
  add_model(spotify_rf_mod) |>
  add_recipe(spotify_rf_recipe)

spotify_rf_tune <- spotify_rf_wf |>
  tune_grid(resamples = spotify_folds, grid = 10, 
            control = ctrl_grid)

spotify_rf_tune |>
  show_best()

spotify_rf_tune |>
  autoplot()
```

For the random forest model, I utilized median and modal imputation to replace missing values in the energy, liveness, loudness, speechiness, liveness, and tempo columns with their medians. Additionally, I used the ranger engine for the random forest model and the classification mode since we are predicting danceability, which has a binary outcome. I used the tune_grid() function to perform hyperparameter tuning. This involves training models with different combinations of hyperparameters and seeing which combination has the highest ROC AUC. The highest ROC AUC for this model is 0.8219402, which is the best performance yet. This occurs at mtry = 1 and min_n = 7. With a roc_auc above 0.8, this model is considered good!

```{r}
#| label: spotify-k-nearest-neighbors
# label: spotify-k-nearest-neighbors

set.seed(100)

spotify_knn_recipe <- recipe(danceability ~ energy + liveness + 
                              loudness + speechiness + tempo, 
                            data = spotify_train) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_naomit(all_outcomes()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())

spotify_knn_mod <- nearest_neighbor() |>
  set_mode("classification") |>
  set_engine("kknn")

spotify_knn_grid <- grid_regular(neighbors(), levels = 10)

spotify_knn_wf <- workflow() |>
  add_model(spotify_knn_mod) |>
  add_recipe(spotify_knn_recipe)

spotify_knn_tune <- spotify_knn_wf |>
  tune_grid(spotify_knn_wf, resamples = spotify_folds, grid = spotify_knn_grid)

spotify_knn_metrics <- spotify_knn_tune |>
  collect_metrics() 

spotify_knn_metrics

```

For the k-nearest neighbor model, I know that it requires all numeric predictors, and all need to be centered and scaled. I used step_dummy() to convert nominal data into numeric dummy variables, which was needed as predictors. I used step_novel() since it "adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set" (Soltoff Better training data lecture). I also used step_zv() to handle zero variance variables. I normalized all numeric predictors and then used step_dummy() on all nominal predictors and then utilized median and modal imputation to replace missing values in the energy, liveness, loudness, speechiness, liveness, and tempo columns. I used kknn as the engine and tuned the neighbors() hyperparameter, which determines the number of neighbors to consider when making predictions. The accuracy for the k-nearest neighbor model is 0.708430 while the roc_auc is 0.756437. With a roc_auc halfway between 0.7 and 0.8, this model performs pretty decently.

```{r}
#| label: spotify-ridge-logistic-regression
# label: spotify-ridge-logistic-regression

set.seed(100)

spotify_ridge_mod <- logistic_reg(penalty = tune(), mixture = tune()) |>
  set_mode("classification") |>
  set_engine("glmnet")

spotify_ridge_grid <- expand_grid(
penalty = 10^seq(-6, -1, length.out = 20),
mixture = c(0, 0.2, 0.4, 0.6, 0.8, 1)
)

spotify_ridge_wf <- workflow() |>
  add_model(spotify_ridge_mod) |>
  add_recipe(spotify_knn_recipe)

spotify_ridge_tune <- tune_grid(spotify_ridge_wf, resamples = spotify_folds, 
                        grid = spotify_ridge_grid)

spotify_ridge_tune |>
  show_best()

spotify_ridge_tune |>
  autoplot()
```

For the ridge logistic regression model, I used the same recipe as the k-nearest neighbor model. I used glmnet as the engine and tuned the penalty and mixture hyperparameters. I tested the penalty parameter at the values 10\^seq(-6, -1, length.out = 20), and tested the mixture parameter at the values c(0, 0.2, 0.4, 0.6, 0.8, 1). The highest roc_auc for this model is 0.7386556, indicating a decent performance.

## Exercise 9

```{r}
#| label: spotify-final-predictive-model
# label: spotify-final-predictive-model

# add code here
spotify_rf_mod_final <- rand_forest(mtry = 1, min_n = 7) |>
  set_mode("classification") |>
  set_engine("ranger")

spotify_rf_wf_final <- workflow() |>
  add_model(spotify_rf_mod_final) |>
  add_recipe(spotify_rf_recipe)

spotify_rf_fit_final <- spotify_rf_wf_final |> 
  fit(data = spotify_train)

bind_cols(
spotify_test,
predict(spotify_rf_fit_final, new_data = spotify_test)
) |>
select(.id, starts_with(".pred")) |>
write_csv(file = "data/spotify-preds.csv")
```

I chose the random model because it performed the best in terms of ROC AUC during the cross-validation process. With the hyperparameters set to the optimal levels, this model performs the best in terms of predicting danceability.
